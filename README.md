The python notebook contain the code to inference LLM using google colab Not the optimal way to use the compute but can be useful to beginners 
TPU LINK : https://colab.research.google.com/github/SanthoshROz4/LLM_Inference_Collab/blob/main/Llama_8_12b_gguf_TPU_LLM_Inference.ipynb

T4 LINK : https://colab.research.google.com/github/SanthoshROz4/LLM_Inference_Collab/blob/main/Llama_8-12b_GGUF_cuda_T4_LLM_Inference.ipynb


Note: inferenced only gguf models 
T4 is optimized for GPU off loading 

TPU notebook wont run efficiently the colab backend just depreacted their versions use T4 so dont spend time on running TPU if possible alter the code of TPU inference

T4 is currently has faster inference speeds
